---
sidebar_position: 5
---

# How to track token usage

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LLMs](/docs/concepts/text_llms)

:::

This notebook goes over how to track your token usage for specific LLM calls. This is only implemented by some providers, including OpenAI. There are two main approaches: accessing token usage directly from the result, or using callbacks.

## Direct access via `result.llmOutput.tokenUsage`

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/llm/token_usage_tracking.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

The recommended approach is to access token usage directly from the `LLMResult` object returned by the `generate()` method. This provides immediate access to token usage information without requiring callbacks:

import DirectExample from "@examples/models/llm/token_usage_tracking_direct.ts";

<CodeBlock language="typescript">{DirectExample}</CodeBlock>

The token usage information is available in the `llmOutput.tokenUsage` field and includes:

- `completionTokens`: Number of tokens used in the generated completion
- `promptTokens`: Number of tokens used in the input prompt
- `totalTokens`: Total number of tokens used (prompt + completion)

:::info LLMs vs Chat Models

Note that LLMs use the `llmOutput.tokenUsage` structure shown above, while chat models use a different approach with `usage_metadata` on `AIMessage` objects. For chat model token usage tracking, see the [chat model token usage guide](/docs/how_to/chat_token_usage_tracking).

:::

## Alternative: Using callbacks

You can also track token usage using callbacks, which is useful when the LLM is called within chains or agents:

<CodeBlock language="typescript">{Example}</CodeBlock>

This callback approach is particularly useful when:

- The LLM is used within chains or agents that call it multiple times
- You want to track token usage across multiple LLM calls
- You need to perform additional processing on each token usage event

If this model is passed to a chain or agent that calls it multiple times, the callback will be triggered each time, allowing you to track cumulative usage.

## Next steps

You've now seen how to get token usage for supported LLM providers.

Next, check out the other how-to guides in this section, like [how to implement your own custom LLM](/docs/how_to/custom_llm).
